{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tET5PUEiDKC6"
      },
      "source": [
        "# Get Ready for the Libraries"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ULStMSdWp4AV"
      },
      "outputs": [],
      "source": [
        "!pip install catboost\n",
        "!pip install yfinance\n",
        "!pip install seaborn\n",
        "!pip install PyPortfolioOpt"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "JUNyR4Ylpr7x"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "import yfinance as yf\n",
        "\n",
        "from math import exp\n",
        "from datetime import date,timedelta\n",
        "from catboost import CatBoostRegressor, Pool\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.linear_model import LinearRegression\n",
        "from pypfopt.risk_models import CovarianceShrinkage\n",
        "from pypfopt import black_litterman, plotting, objective_functions\n",
        "from pypfopt.black_litterman import BlackLittermanModel\n",
        "from pypfopt.efficient_frontier import EfficientFrontier"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4g5F__kADKD5"
      },
      "source": [
        "# Get Ready for the datasets"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "cwpaoyh47CCJ",
        "outputId": "8441580b-393a-41ea-e501-5c0a1bd432c6"
      },
      "outputs": [],
      "source": [
        "!gdown --id 1qLoKEZHEjqvjgBB1CFX62oh7IGJptAk7\n",
        "!gdown --id 1YZmaQNzgpkj-DHbyFUhzzSDXKajabViF\n",
        "!gdown --id 1KRYPKo4ZEZdbq0RCmIutu6n93FYAjQDN"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SHyFsKAj5NAR"
      },
      "source": [
        "# Import data from Yahoo Finance"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "gJhlHSrz5K4I"
      },
      "outputs": [],
      "source": [
        "#list of stock codes of S&P stocks\n",
        "tickers_csv = pd.read_csv(\"S&P500.csv\")\n",
        "tickers = list(tickers_csv[\"Symbol\"])\n",
        "\n",
        "#eliminate stocks that are too new and have insuffcient traning data\n",
        "bug_stocks = [\"BRK.B\",\"BF.B\",\"EMBC\",\"CEG\",\"OGN\",\"CARR\",\"OTIS\",\"CTVA\",\"MRNA\",\"FOX\",\"FOXA\",\"DOW\",\"CDAY\",\"IR\"]\n",
        "tickers = list(set(tickers) - set(bug_stocks))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "yTRnNeRd5bE1",
        "outputId": "0d320288-5834-47d4-af9f-19506feef95d"
      },
      "outputs": [],
      "source": [
        "# Get AdjClose, Close, High, Low, Open, Volume\n",
        "yf.pdr_override()\n",
        "end = pd.to_datetime(\"30/04/2022\")\n",
        "start = end - timedelta(days=5000)\n",
        "data = yf.download(tickers, start=start, end=end,group_by=\"ticker\", interval=\"1d\")\n",
        "\n",
        "data_to_process = pd.DataFrame.copy(data)\n",
        "data_processed = data_to_process.dropna(axis=0)\n",
        "data_processed"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "hjMxnZ4t5eps",
        "outputId": "123591b3-1c96-47e5-f043-91f23d64cdde"
      },
      "outputs": [],
      "source": [
        "#Get information on SnP for comparison\n",
        "SnP_start=data_processed.index[0].date()\n",
        "SnP_end=date.today()\n",
        "SnP=yf.download('^GSPC',start=SnP_start, end=SnP_end, interval='1d')\n",
        "\n",
        "#Calculate log daily log return of S&P\n",
        "SnP['SnP Log Return']=np.log(SnP['Close'])-np.log(SnP['Close']).shift(1)\n",
        "SnP"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_WGwflra5ien"
      },
      "source": [
        "# Feature Engineering"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "INsOwxDD5jkY"
      },
      "outputs": [],
      "source": [
        "def find_std(data,period,start_date):\n",
        "\n",
        "    df = data.copy()\n",
        "    end = df.index.get_loc(start_date)\n",
        "    start=end-period\n",
        "    if start>=0:\n",
        "        log_return=df['Log Return'].iloc[start:end]\n",
        "        std=np.std(log_return, ddof=1)\n",
        "        return std\n",
        "    else:\n",
        "        return np.NaN\n",
        "\n",
        "\n",
        "def find_beta(data,period,start_date):\n",
        "\n",
        "    df = data.copy()\n",
        "    end=df.index.get_loc(start_date)\n",
        "    start=end-period\n",
        "    if start>=0:\n",
        "        log_return=df['Log Return'].iloc[start:end].to_numpy()\n",
        "        SnP_log_return=df['SnP Log Return'].iloc[start:end].to_numpy().reshape(-1,1)\n",
        "\n",
        "        regr = LinearRegression()\n",
        "        beta=regr.fit(SnP_log_return,log_return).coef_[0]\n",
        "        return beta\n",
        "    else:\n",
        "        return np.NaN\n",
        "\n",
        "def ShiftNum(var):\n",
        "    if var[1]=='D':\n",
        "        return int(var[0])\n",
        "    elif var[1]=='W':\n",
        "        return int(var[0])*5\n",
        "    elif var[1]=='M':\n",
        "        return int(var[0])*20\n",
        "    elif var[1]=='Y':\n",
        "        return int(var[0])*250\n",
        "    else:\n",
        "        return(\"Please give a str whose format is xD, xW, xM, xY.\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "PzVS4nEL5nCf"
      },
      "outputs": [],
      "source": [
        "stocks={}\n",
        "return_lag=['1D','3D','1W','2W','3W','1M','6W','2M','3M']\n",
        "for stock in tickers:\n",
        "    stocks[stock]=data_processed[stock][['Close']].copy()\n",
        "\n",
        "    #retain stock code as a feature\n",
        "    stocks[stock]['Stock']=[stock]*stocks[stock].shape[0]\n",
        "\n",
        "    #daily log return of stocks\n",
        "    stocks[stock]['Log Return']=np.log(stocks[stock]['Close'])-np.log(stocks[stock]['Close']).shift(1)\n",
        "\n",
        "    #join with SnP return for generating other features \n",
        "    #(SnP return on that day will not be used as feature to avoid data leakage)\n",
        "    stocks[stock]=stocks[stock].join(SnP[['SnP Log Return']], how='left')\n",
        "\n",
        "    #SnP return of yesterday as a feature\n",
        "    stocks[stock]['SnP Log Return_1D']=stocks[stock]['SnP Log Return'].shift(1)\n",
        "    stocks[stock].dropna(inplace=True)\n",
        "\n",
        "    #calculate s.d. of stock return in the past 30 days\n",
        "    stocks[stock]['Volatility']=[ find_std(stocks[stock],30,date) for date in stocks[stock].index]\n",
        "\n",
        "    #calculate beta(a measure of a stock's volatility in relation to the overall market) for the past 30days\n",
        "    stocks[stock]['Beta']=[ find_beta(stocks[stock],30,date) for date in stocks[stock].index]\n",
        "\n",
        "    #Use return from n_days ago as feature\n",
        "    for var in return_lag:\n",
        "        name = 'Return_' + var\n",
        "        stocks[stock][name] =  stocks[stock]['Log Return'].shift(ShiftNum(var))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "p-S9Vl7i5pVL"
      },
      "outputs": [],
      "source": [
        "#join information of all stocks in one df\n",
        "cleaned=pd.DataFrame()\n",
        "for stock in tickers:\n",
        "    cleaned=pd.concat([cleaned,stocks[stock]])\n",
        "cleaned.dropna(inplace=True)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZGUNIAwDpr7y"
      },
      "source": [
        "# Data Processing"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "q2v6s6Dnpr7z"
      },
      "outputs": [],
      "source": [
        "# Read csv Files\n",
        "riskfree = pd.read_csv('risk_free.csv')\n",
        "market_cap = pd.read_csv(\"market_cap.csv\").astype(\"int64\")\n",
        "\n",
        "# For cleaned\n",
        "cleaned.index = pd.to_datetime(cleaned.index)\n",
        "SnP_Return = cleaned[[\"SnP Log Return\"]]\n",
        "cleaned = cleaned.drop(columns=[\"SnP Log Return\",\"Close\"],axis=1)\n",
        "cleaned = cleaned.sort_values(by=[\"Stock\",\"Date\"])\n",
        "\n",
        "# For Market Capitalisation\n",
        "market_cap = market_cap.iloc[0]\n",
        "market_cap = market_cap.to_dict()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "LXzFzhtYpr7z"
      },
      "outputs": [],
      "source": [
        "# Get list[date] for later looping\n",
        "date_list = cleaned.index\n",
        "date_list = date_list.drop_duplicates()\n",
        "stock_list = cleaned[\"Stock\"]\n",
        "stock_list = stock_list.drop_duplicates()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "VjtTVxnw4vVY"
      },
      "outputs": [],
      "source": [
        "SnP_Return = SnP_Return.drop_duplicates()\n",
        "SnP_Return = SnP_Return.reset_index().drop_duplicates().set_index(\"Date\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ZCe9r07-pr72"
      },
      "outputs": [],
      "source": [
        "log_return_df = cleaned[['Stock','Log Return']].reset_index().set_index(['Stock','Date']).unstack(level=[0])\n",
        "log_return_df = log_return_df[\"Log Return\"]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "WXRhcixKJPPA",
        "outputId": "1149ccfe-b2e2-4da3-fb78-772fdfc60f59"
      },
      "outputs": [],
      "source": [
        "# Import SPY Data from API\n",
        "SPY = yf.download('SPY', start=date_list[0], end=date_list[-1])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BLPTw661DKEk"
      },
      "source": [
        "# Main Body for Machine Learning & Optimisation"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2gDrLnPgpr73",
        "outputId": "3da30d09-2500-480b-d49c-611c5d447105"
      },
      "outputs": [],
      "source": [
        "# Adjustible parameters\n",
        "n_days=252\n",
        "training_len=100\n",
        "\n",
        "# Pre-set parameters\n",
        "trade_days=date_list[-n_days:]\n",
        "daily_return = []\n",
        "ms_daily_return = []\n",
        "mv_daily_return = []\n",
        "seed=0\n",
        "\n",
        "# Looping\n",
        "for trade_day in trade_days:\n",
        "    print(trade_day)\n",
        "    trade_day_index=date_list.get_loc(trade_day)\n",
        "    first_training_day_index=trade_day_index-training_len\n",
        "    train_valid_dates=date_list[first_training_day_index:trade_day_index]\n",
        "    \n",
        "    # Train Validation Split\n",
        "    train_days, eval_days=train_test_split(train_valid_dates,test_size=0.7,random_state=seed)\n",
        "    seed+=1\n",
        "\n",
        "    train_data=cleaned.loc[train_days,:]\n",
        "    eval_data=cleaned.loc[eval_days,:]\n",
        "    trade_day_data = cleaned.loc[trade_day]\n",
        "\n",
        "    train_x=train_data.drop(['Log Return'],axis=1)\n",
        "    train_y=train_data['Log Return']\n",
        "    eval_x=eval_data.drop(['Log Return'],axis=1)\n",
        "    eval_y=eval_data['Log Return']\n",
        "    trade_day_x=trade_day_data.drop(['Log Return'],axis=1)\n",
        "    trade_day_y=trade_day_data['Log Return']\n",
        "\n",
        "    # Establish CatBoost Model \n",
        "    model=CatBoostRegressor(iterations=100,task_type=\"CPU\",learning_rate=0.1,\n",
        "                depth=8,l2_leaf_reg=0.0000001,allow_writing_files=False,\n",
        "                eval_metric='MAPE',random_seed=0,thread_count=-1)\n",
        "    eval_set=Pool(eval_x,eval_y,cat_features=[0])\n",
        "\n",
        "    catboost_train_data = Pool(data=train_x,label=train_y,cat_features=[0])\n",
        "    model.fit(catboost_train_data,\n",
        "              eval_set=eval_set,\n",
        "              plot=False,\n",
        "              early_stopping_rounds=10)\n",
        "    \n",
        "    # Prediction from CatBoost Model\n",
        "    preds_log_return=model.predict(trade_day_x)\n",
        "\n",
        "    # Append Predicted Array to Log Return DataFrame\n",
        "    temp_df = pd.DataFrame(preds_log_return).transpose()\n",
        "    temp_df.columns = stock_list\n",
        "    log_return_opt_df = log_return_df[first_training_day_index:trade_day_index]\n",
        "    log_return_opt_df = log_return_opt_df.append(temp_df)\n",
        "    log_return_opt_df.index = date_list[first_training_day_index:trade_day_index+1]\n",
        "\n",
        "    # Ready for Optimisation\n",
        "    portfolio = log_return_opt_df.applymap(lambda x: exp(x))\n",
        "    cs_actual = CovarianceShrinkage(portfolio, frequency=len(log_return_opt_df))\n",
        "    e_cov = cs_actual.ledoit_wolf()\n",
        "\n",
        "    # Prepare Variables for Black Litterman Model\n",
        "    market_prices = SPY.loc[log_return_opt_df.index[0]:log_return_opt_df.index[-2]]\n",
        "    annual_risk_free = riskfree.loc[trade_day_index]['Price']/100\n",
        "    daily_risk_free = (1+annual_risk_free)**(1/252)-1\n",
        "    delta = black_litterman.market_implied_risk_aversion(market_prices['Close'], risk_free_rate=daily_risk_free)\n",
        "    prior = black_litterman.market_implied_prior_returns(market_cap, delta, e_cov)\n",
        "    viewdict={trade_day_x['Stock'].values[i]:exp(preds_log_return[i])-1 for i in range(len(preds_log_return))}\n",
        "    bl = BlackLittermanModel(e_cov, pi=prior, absolute_views=viewdict)\n",
        "    # Posterior estimate of returns\n",
        "    ret_bl = bl.bl_returns()\n",
        "    S_bl = bl.bl_cov()\n",
        "\n",
        "    # Perform Maximum Sharpe Ratio Portfolio\n",
        "    ms_ef = EfficientFrontier(ret_bl, S_bl,verbose=False)\n",
        "    ms_ef.add_objective(objective_functions.L2_reg)\n",
        "\n",
        "    # Decide Method of Optimisation by Error\n",
        "    try:\n",
        "      ms_ef.max_sharpe()\n",
        "      ms_weights = ms_ef.clean_weights()\n",
        "    except:\n",
        "      ms_weights = ms_ef.nonconvex_objective(objective_functions.sharpe_ratio,\n",
        "                          objective_args=(ms_ef.expected_returns, ms_ef.cov_matrix),\n",
        "                          weights_sum_to_one=True,)\n",
        "    ms_weights = list(ms_weights.values())\n",
        "    ms_expected_return = ms_weights@temp_df.iloc[0].to_numpy().T\n",
        "    ms_return = ms_weights@trade_day_y.to_numpy().T\n",
        "    ms_daily_return = np.append(ms_daily_return,ms_return)\n",
        "\n",
        "    # Perform Minimum Volatility Portfolio\n",
        "    mv_ef = EfficientFrontier(ret_bl, S_bl)\n",
        "    mv_ef.add_objective(objective_functions.L2_reg)\n",
        "    mv_ef.min_volatility()\n",
        "    mv_weights = mv_ef.clean_weights()\n",
        "\n",
        "    mv_weights = list(mv_weights.values())\n",
        "    mv_expected_return = mv_weights@temp_df.iloc[0].to_numpy().T\n",
        "    mv_return = mv_weights@trade_day_y.to_numpy().T\n",
        "    mv_daily_return = np.append(mv_daily_return,mv_return)\n",
        "\n",
        "    # Adopt Better Prediction from Maximum Sharpe Ratio Portfolio or Minimum Volatility Portfolio\n",
        "    if (ms_expected_return>mv_expected_return):\n",
        "      daily_return = np.append(daily_return,ms_return)\n",
        "    else:\n",
        "      daily_return = np.append(daily_return,mv_return)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_tRA9QpU64bK",
        "outputId": "d80ae3ee-cf89-4370-fb80-f83de1bc611f"
      },
      "outputs": [],
      "source": [
        "# Ready for Result Exhibition\n",
        "ms_total_return = np.prod(ms_daily_return+1)-1\n",
        "mv_total_return = np.prod(mv_daily_return+1)-1\n",
        "total_return = np.prod(daily_return+1)-1\n",
        "SnP_array = np.array(SnP_Return[\"SnP Log Return\"].loc[trade_days])\n",
        "SnP_total_return = np.prod(SnP_array+1)-1\n",
        "\n",
        "ms_cum_return = np.cumprod(ms_daily_return+1)\n",
        "mv_cum_return = np.cumprod(mv_daily_return+1)\n",
        "cum_return = np.cumprod(daily_return+1)\n",
        "SnP_cum_return = np.cumprod(SnP_array+1)\n",
        "\n",
        "print(ms_cum_return[-1], mv_cum_return[-1], cum_return[-1], SnP_cum_return[-1])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8KmQudjFDCdw",
        "outputId": "2780a55b-903b-42ab-c82b-f3e87e59c499"
      },
      "outputs": [],
      "source": [
        "# Calculate Information Ratio\n",
        "avg_return_algorithm = np.array(daily_return).mean()\n",
        "avg_return_SnP = SnP_array.mean()\n",
        "tracking_error = np.std(np.array(daily_return)-SnP_array,ddof=1)\n",
        "IR = (avg_return_algorithm-avg_return_SnP)/tracking_error\n",
        "annual_IR = (avg_return_algorithm-avg_return_SnP)/tracking_error*252**(1/2)\n",
        "print(IR,annual_IR)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vgNaBnVcFULN"
      },
      "source": [
        "# Result Exhibition"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 550
        },
        "id": "E18HLKh2bFu-",
        "outputId": "59d1f568-9584-4166-e735-4c06ff917b0e"
      },
      "outputs": [],
      "source": [
        "plt.style.use('ggplot')\n",
        "sns.set(rc={'figure.figsize':(11.7,8.27)})\n",
        "\n",
        "plt.xlabel(\"Date\")\n",
        "plt.ylabel(\"Return\")\n",
        "plt.title(\"Portfolios performance\")\n",
        "plt.legend(loc=\"upper left\")\n",
        "plt.plot(trade_days,ms_cum_return,label=\"Maximised Sharpe Ratio\")\n",
        "plt.plot(trade_days,mv_cum_return,label=\"Minimised volatility\")\n",
        "plt.plot(trade_days,cum_return,label=\"Algorithm\")\n",
        "plt.plot(trade_days,SnP_cum_return,c=\"Red\",label=\"S&P500 Index\")\n",
        "plt.legend(loc=\"upper left\")\n",
        "\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "LHWZtToL9d3-"
      },
      "outputs": [],
      "source": [
        "result = pd.DataFrame({'ms_daily_return':ms_daily_return,'mv_daily_return':mv_daily_return,'algo_daily_return':daily_return,'SnP_daily_return':SnP_array})\n",
        "result.index = trade_days"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ridpo7BB-Lqa",
        "outputId": "63a2c124-4b7b-4c28-fd97-3a7281f78bd1"
      },
      "outputs": [],
      "source": [
        "from google.colab import  drive\n",
        "\n",
        "drive.mount('/drive',force_remount=True)\n",
        "result.to_csv('/drive/My Drive/result_20220510_2302.csv')"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "collapsed_sections": [],
      "name": "Finalized_Code_Trial2.ipynb",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3.9.7 ('base')",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.9.7"
    },
    "vscode": {
      "interpreter": {
        "hash": "750f7130a0ed5887341392c7bdb5265c945b64bfd064bdaec7ae8d3d01e224ab"
      }
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
