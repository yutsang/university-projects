{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<p style=\"text-align:center\">\n",
    "        <img src=\"https://cf-courses-data.s3.us.cloud-object-storage.appdomain.cloud/assets/logos/SN_web_lightmode.png\" width=\"300\" alt=\"Skills Network Logo\">\n",
    "</p>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Analyse search terms on the e-commerce web server\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### In this assignment you will download the search term data set for the e-commerce web server and run analytic queries on it.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Install spark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "37\n",
      "Starting installation...\n",
      "Please <<RESTART YOUR KERNEL>> (Kernel->Restart Kernel and Clear All Outputs)\n"
     ]
    }
   ],
   "source": [
    "%%bash\n",
    "export version=`python --version |awk '{print $2}' |awk -F\".\" '{print $1$2}'`\n",
    "\n",
    "echo $version\n",
    "\n",
    "if [ $version == '36' ] || [ $version == '37' ]; then\n",
    "    echo 'Starting installation...'\n",
    "    pip3 install pyspark==2.4.8 wget==3.2 pyspark2pmml==0.5.1 > install.log 2> install.log\n",
    "    if [ $? == 0 ]; then\n",
    "        echo 'Please <<RESTART YOUR KERNEL>> (Kernel->Restart Kernel and Clear All Outputs)'\n",
    "    else\n",
    "        echo 'Installation failed, please check log:'\n",
    "        cat install.log\n",
    "    fi\n",
    "elif [ $version == '38' ] || [ $version == '39' ]; then\n",
    "    pip3 install pyspark==3.1.2 wget==3.2 pyspark2pmml==0.5.1 > install.log 2> install.log\n",
    "    if [ $? == 0 ]; then\n",
    "        echo 'Please <<RESTART YOUR KERNEL>> (Kernel->Restart Kernel and Clear All Outputs)'\n",
    "    else\n",
    "        echo 'Installation failed, please check log:'\n",
    "        cat install.log\n",
    "    fi\n",
    "else\n",
    "    echo 'Currently only python 3.6, 3.7 , 3.8 and 3.9 are supported, in case you need a different version please open an issue at https://github.com/IBM/claimed/issues'\n",
    "    exit -1\n",
    "fi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Start session"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from pyspark import SparkContext, SparkConf, SQLContext\n",
    "from pyspark.sql import SparkSession\n",
    "import os\n",
    "from pyspark.ml.classification import RandomForestClassifier\n",
    "from pyspark.sql.types import DoubleType\n",
    "from pyspark.ml.evaluation import MulticlassClassificationEvaluator\n",
    "from pyspark.ml import Pipeline\n",
    "# from pyspark2pmml import PMMLBuilder\n",
    "from pyspark.ml.feature import StringIndexer\n",
    "from pyspark.ml.feature import VectorAssembler\n",
    "from pyspark.ml.feature import MinMaxScaler\n",
    "import logging\n",
    "import shutil\n",
    "import site\n",
    "import sys\n",
    "import wget\n",
    "import re\n",
    "import pandas as pd\n",
    "import glob"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "if sys.version[0:3] == '3.9':\n",
    "    url = ('https://github.com/jpmml/jpmml-sparkml/releases/download/1.7.2/'\n",
    "           'jpmml-sparkml-executable-1.7.2.jar')\n",
    "    wget.download(url)\n",
    "    shutil.copy('jpmml-sparkml-executable-1.7.2.jar',\n",
    "                site.getsitepackages()[0] + '/pyspark/jars/')\n",
    "elif sys.version[0:3] == '3.8':\n",
    "    url = ('https://github.com/jpmml/jpmml-sparkml/releases/download/1.7.2/'\n",
    "           'jpmml-sparkml-executable-1.7.2.jar')\n",
    "    wget.download(url)\n",
    "    shutil.copy('jpmml-sparkml-executable-1.7.2.jar',\n",
    "                site.getsitepackages()[0] + '/pyspark/jars/')\n",
    "elif sys.version[0:3] == '3.7':\n",
    "    url = ('https://github.com/jpmml/jpmml-sparkml/releases/download/1.5.12/'\n",
    "           'jpmml-sparkml-executable-1.5.12.jar')\n",
    "    wget.download(url)\n",
    "elif sys.version[0:3] == '3.6':\n",
    "    url = ('https://github.com/jpmml/jpmml-sparkml/releases/download/1.5.12/'\n",
    "           'jpmml-sparkml-executable-1.5.12.jar')\n",
    "    wget.download(url)\n",
    "else:\n",
    "    raise Exception('Currently only python 3.6 , 3.7, 3,8 and 3.9 is supported, in case '\n",
    "                    'you need a different version please open an issue at '\n",
    "                    'https://github.com/IBM/claimed/issues')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Download The search term dataset from the below url\n",
    "# https://cf-courses-data.s3.us.cloud-object-storage.appdomain.cloud/IBM-DB0321EN-SkillsNetwork/Bigdata%20and%20Spark/searchterms.csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import urllib.request\n",
    "\n",
    "# Download CSV file from URL\n",
    "url = 'https://cf-courses-data.s3.us.cloud-object-storage.appdomain.cloud/IBM-DB0321EN-SkillsNetwork/Bigdata%20and%20Spark/searchterms.csv'\n",
    "filename = 'searchterms.csv'\n",
    "urllib.request.urlretrieve(url, filename)\n",
    "\n",
    "# Load CSV file into Pandas DataFrame\n",
    "df = pd.read_csv(filename)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "23/07/21 07:00:31 WARN util.NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n",
      "Setting default log level to \"WARN\".\n",
      "To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n",
      "[Stage 0:>                                                          (0 + 8) / 8]23/07/21 07:00:49 WARN hadoop.MemoryManager: Total allocation exceeds 95.00% (906,992,014 bytes) of heap memory\n",
      "Scaling row group sizes to 96.54% for 7 writers\n",
      "23/07/21 07:00:49 WARN hadoop.MemoryManager: Total allocation exceeds 95.00% (906,992,014 bytes) of heap memory\n",
      "Scaling row group sizes to 84.47% for 8 writers\n",
      "23/07/21 07:00:49 WARN hadoop.MemoryManager: Total allocation exceeds 95.00% (906,992,014 bytes) of heap memory\n",
      "Scaling row group sizes to 96.54% for 7 writers\n",
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "master = os.environ.get('master',\n",
    "                        \"local[*]\")  # URL to Spark master\n",
    "\n",
    "conf = SparkConf().setMaster(master)\n",
    "#if sys.version[0:3] == '3.6' or sys.version[0:3] == '3.7':\n",
    "conf.set(\"spark.jars\", 'jpmml-sparkml-executable-1.5.12.jar')\n",
    "\n",
    "sc = SparkContext.getOrCreate(conf)\n",
    "sqlContext = SQLContext(sc)\n",
    "spark = sqlContext.sparkSession\n",
    "\n",
    "# Convert Pandas DataFrame to Spark DataFrame\n",
    "sdf = spark.createDataFrame(df)\n",
    "# Write Spark DataFrame to Parquet format\n",
    "sdf.write.parquet('output.parquet')\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the csv into a spark dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Print the number of rows and columns\n",
    "# Take a screenshot of the code and name it as shape.jpg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of rows: day           10000\n",
      "month         10000\n",
      "year          10000\n",
      "searchterm    10000\n",
      "dtype: int64\n",
      "Number of columns: 4\n"
     ]
    }
   ],
   "source": [
    "# Get number of rows and columns\n",
    "num_rows = df.count()\n",
    "num_cols = len(df.columns)\n",
    "\n",
    "# Print results\n",
    "print(\"Number of rows: {}\".format(num_rows))\n",
    "print(\"Number of columns: {}\".format(num_cols))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Print the top 5 rows\n",
    "# Take a screenshot of the code and name it as top5rows.jpg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+-----+----+--------------+\n",
      "|day|month|year|    searchterm|\n",
      "+---+-----+----+--------------+\n",
      "| 12|   11|2021| mobile 6 inch|\n",
      "| 12|   11|2021| mobile latest|\n",
      "| 12|   11|2021|   tablet wifi|\n",
      "| 12|   11|2021|laptop 14 inch|\n",
      "| 12|   11|2021|     mobile 5g|\n",
      "+---+-----+----+--------------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "sdf.show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Find out the datatype of the column searchterm?\n",
    "# Take a screenshot of the code and name it as datatype.jpg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "pyspark.sql.column.Column"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(sdf['searchterm'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# How many times was the term `gaming laptop` searched?\n",
    "# Take a screenshot of the code and name it as gaminglaptop.jpg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "data": {
      "text/plain": [
       "499"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sdf.filter(sdf['searchterm'] == 'gaming laptop').count ()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Print the top 5 most frequently used search terms?\n",
    "# Take a screenshot of the code and name it as top5terms.jpg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 5:=============================>                        (108 + 13) / 200]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------+-----------------+\n",
      "|   searchterm|count(searchterm)|\n",
      "+-------------+-----------------+\n",
      "|mobile 6 inch|             2312|\n",
      "|    mobile 5g|             2301|\n",
      "|mobile latest|             1327|\n",
      "|       laptop|              935|\n",
      "|  tablet wifi|              896|\n",
      "+-------------+-----------------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "counts = sdf.groupby (['searchterm']).agg({\"searchterm\": \"count\"}).sort (\"count(searchterm)\", ascending=False)\n",
    "\n",
    "counts.show (5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# The pretrained sales forecasting model is available at  the below url\n",
    "# https://cf-courses-data.s3.us.cloud-object-storage.appdomain.cloud/IBM-DB0321EN-SkillsNetwork/Bigdata%20and%20Spark/model.tar.gz"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('model.tar.gz', <http.client.HTTPMessage at 0x7f328379db10>)"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import urllib.request\n",
    "\n",
    "# Download model file from URL\n",
    "url = 'https://cf-courses-data.s3.us.cloud-object-storage.appdomain.cloud/IBM-DB0321EN-SkillsNetwork/Bigdata%20and%20Spark/model.tar.gz'\n",
    "filename = 'model.tar.gz'\n",
    "urllib.request.urlretrieve(url, filename)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the sales forecast model.\n",
    "# Take a screenshot of the code and name it as loadmodel.jpg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "model = LinearRegressionModel.load('sales_prediction.model')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Using the sales forecast model, predict the sales for the year of 2023.\n",
    "# Take a screenshot of the code and name it as forecast.jpg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------------------+\n",
      "|        prediction|\n",
      "+------------------+\n",
      "|175.16564294006457|\n",
      "+------------------+\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "23/07/21 07:38:12 WARN netlib.BLAS: Failed to load implementation from: com.github.fommil.netlib.NativeSystemBLAS\n",
      "23/07/21 07:38:12 WARN netlib.BLAS: Failed to load implementation from: com.github.fommil.netlib.NativeRefBLAS\n"
     ]
    }
   ],
   "source": [
    "from pyspark.ml.feature import VectorAssembler\n",
    "\n",
    "def predict(year):\n",
    "    assembler = VectorAssembler(inputCols=[\"year\"], outputCol=\"features\")\n",
    "    data = [[0, 0, year, 0]]\n",
    "    columns = [\"day\", \"month\", \"year\", \"search\"]\n",
    "    df = spark.createDataFrame(data, columns) \n",
    "    df = assembler.transform(df).select('features', 'year')\n",
    "    predictions = model.transform(df)\n",
    "    predictions.select('prediction').show()\n",
    "\n",
    "predict(2023)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python",
   "language": "python",
   "name": "conda-env-python-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
