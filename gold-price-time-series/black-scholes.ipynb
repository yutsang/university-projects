{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Date parsing error: time data \"Date\" doesn't match format \"%Y-%m-%d %H:%M:%S%z\", at position 0. You might want to try:\n",
      "    - passing `format` if your strings have a consistent format;\n",
      "    - passing `format='ISO8601'` if your strings are all ISO8601 but not necessarily in exactly the same format;\n",
      "    - passing `format='mixed'`, and the format will be inferred for each element individually. You might want to use `dayfirst` alongside this.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/rx/s42f27kn5j5ddqzlhz7qr2yw0000gn/T/ipykernel_12571/2874450906.py:28: FutureWarning: The argument 'date_parser' is deprecated and will be removed in a future version. Please use 'date_format' instead, or read your data in as 'object' dtype and then call 'to_datetime'.\n",
      "  df = pd.read_csv(file_path, skiprows=2,\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'df' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[4], line 38\u001b[0m\n\u001b[1;32m     35\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mDate parsing error:\u001b[39m\u001b[38;5;124m\"\u001b[39m, e)\n\u001b[1;32m     37\u001b[0m \u001b[38;5;66;03m# Filter to keep only essential columns and drop rows with NaN values in 'Price'\u001b[39;00m\n\u001b[0;32m---> 38\u001b[0m df \u001b[38;5;241m=\u001b[39m \u001b[43mdf\u001b[49m[[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mDate\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mPrice\u001b[39m\u001b[38;5;124m'\u001b[39m]]\u001b[38;5;241m.\u001b[39mdropna()\n\u001b[1;32m     40\u001b[0m \u001b[38;5;66;03m# Convert 'Price' to a numeric type to ensure proper data handling\u001b[39;00m\n\u001b[1;32m     41\u001b[0m df[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mPrice\u001b[39m\u001b[38;5;124m'\u001b[39m] \u001b[38;5;241m=\u001b[39m pd\u001b[38;5;241m.\u001b[39mto_numeric(df[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mPrice\u001b[39m\u001b[38;5;124m'\u001b[39m], errors\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mcoerce\u001b[39m\u001b[38;5;124m'\u001b[39m)\n",
      "\u001b[0;31mNameError\u001b[0m: name 'df' is not defined"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from scipy.stats import norm\n",
    "from scipy.optimize import minimize\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import LSTM, Dense, Dropout\n",
    "from sklearn.metrics import mean_squared_error\n",
    "import tensorflow as tf\n",
    "\n",
    "# Step 1: Configure TensorFlow for memory management\n",
    "gpus = tf.config.list_physical_devices('GPU')\n",
    "if gpus:\n",
    "    try:\n",
    "        for gpu in gpus:\n",
    "            tf.config.experimental.set_memory_growth(gpu, True)\n",
    "    except RuntimeError as e:\n",
    "        print(e)\n",
    "\n",
    "# Set mixed precision policy for reduced memory usage and faster computation\n",
    "tf.keras.mixed_precision.set_global_policy('mixed_float16')\n",
    "\n",
    "# Step 2: Load and preprocess data with error handling and optimized date parsing\n",
    "file_path = 'gold_dec24-GC-F-_1wk.csv'\n",
    "try:\n",
    "    # Load the CSV file and parse dates using ISO8601 format\n",
    "    df = pd.read_csv(file_path, skiprows=2,\n",
    "                     names=['Date', 'Price', 'Adj Close', 'Close', 'High', 'Low', 'Open', 'Volume'],\n",
    "                     parse_dates=['Date'], \n",
    "                     date_parser=lambda x: pd.to_datetime(x.strip(), format='%Y-%m-%d %H:%M:%S%z'))\n",
    "except FileNotFoundError:\n",
    "    raise Exception(f\"File not found: {file_path}\")\n",
    "except ValueError as e:\n",
    "    print(\"Date parsing error:\", e)\n",
    "\n",
    "# Filter to keep only essential columns and drop rows with NaN values in 'Price'\n",
    "df = df[['Date', 'Price']].dropna()\n",
    "\n",
    "# Convert 'Price' to a numeric type to ensure proper data handling\n",
    "df['Price'] = pd.to_numeric(df['Price'], errors='coerce')\n",
    "\n",
    "# Check the shape of the DataFrame after filtering\n",
    "print(f\"Data shape after filtering: {df.shape}\")\n",
    "\n",
    "# Step 3: Define Black-Scholes model (remains unchanged)\n",
    "def black_scholes(S, K, T, r, sigma):\n",
    "    d1 = (np.log(S / K) + (r + 0.5 * sigma ** 2) * T) / (sigma * np.sqrt(T))\n",
    "    d2 = d1 - sigma * np.sqrt(T)\n",
    "    call_price = S * norm.cdf(d1) - K * np.exp(-r * T) * norm.cdf(d2)\n",
    "    return call_price\n",
    "\n",
    "# Set up parameters for Black-Scholes\n",
    "S = df['Price'].values  \n",
    "K = np.mean(S)                 \n",
    "T = 1                          \n",
    "r = 0.01                       \n",
    "\n",
    "# Define objective function to estimate volatility (remains unchanged)\n",
    "def objective_function(sigma):\n",
    "    call_prices = black_scholes(S, K, T, r, sigma)\n",
    "    return np.mean((call_prices - S) ** 2)\n",
    "\n",
    "# Estimate volatility using optimization (remains unchanged)\n",
    "initial_sigma = 0.2\n",
    "result = minimize(objective_function, initial_sigma, bounds=[(0.01, 1)])\n",
    "estimated_sigma = result.x[0]   \n",
    "\n",
    "# Step 4: Prepare data for LSTM model (Deep Learning)\n",
    "scaler = MinMaxScaler(feature_range=(0, 1))\n",
    "scaled_data = scaler.fit_transform(df[['Price']])\n",
    "\n",
    "train_size = int(len(scaled_data) * 0.8)\n",
    "train_data = scaled_data[:train_size]\n",
    "test_data = scaled_data[train_size:]\n",
    "\n",
    "def create_sequences(data, look_back=60):\n",
    "    X, y = [], []\n",
    "    for i in range(look_back, len(data)):\n",
    "        X.append(data[i-look_back:i])\n",
    "        y.append(data[i])\n",
    "    return np.array(X), np.array(y)\n",
    "\n",
    "look_back = 60\n",
    "X_train, y_train = create_sequences(train_data, look_back)\n",
    "X_test, y_test = create_sequences(test_data, look_back)\n",
    "\n",
    "X_train = X_train.reshape(X_train.shape[0], X_train.shape[1], 1)\n",
    "X_test = X_test.reshape(X_test.shape[0], X_test.shape[1], 1)\n",
    "\n",
    "# Step 5: Build LSTM Model with improved architecture (remains unchanged)\n",
    "model = Sequential()\n",
    "model.add(LSTM(units=50, return_sequences=True, input_shape=(X_train.shape[1], 1)))\n",
    "model.add(Dropout(0.2))\n",
    "model.add(LSTM(units=25))  # Reduced units to save memory\n",
    "model.add(Dropout(0.2))\n",
    "model.add(Dense(units=1))\n",
    "\n",
    "model.compile(optimizer='adam', loss='mean_squared_error')\n",
    "\n",
    "# Step 6: Create efficient data pipelines using tf.data API (remains unchanged)\n",
    "train_dataset = tf.data.Dataset.from_tensor_slices((X_train, y_train)).batch(32).prefetch(tf.data.AUTOTUNE)\n",
    "\n",
    "# Step 7: Train the LSTM Model with early stopping (remains unchanged)\n",
    "from tensorflow.keras.callbacks import EarlyStopping\n",
    "\n",
    "early_stopping = EarlyStopping(monitor='loss', patience=5)\n",
    "model.fit(train_dataset, epochs=50, callbacks=[early_stopping])\n",
    "\n",
    "# Step 8: Predict future prices using LSTM model on test set (remains unchanged)\n",
    "predicted_prices_lstm = model.predict(X_test)\n",
    "predicted_prices_lstm_scaled_back = scaler.inverse_transform(predicted_prices_lstm)\n",
    "\n",
    "# Evaluate model performance on test set (remains unchanged)\n",
    "mse_test = mean_squared_error(df['Price'][train_size + look_back:], predicted_prices_lstm_scaled_back.flatten())\n",
    "print(f'Mean Squared Error on Test Set: {mse_test:.2f}')\n",
    "\n",
    "# Step 9: Predict next four weeks (28 days) of prices based on last known data point (remains unchanged)\n",
    "last_sequence = test_data[-look_back:]\n",
    "forecasted_prices_lstm_scaled_back = []\n",
    "for _ in range(28):  \n",
    "    last_sequence_reshaped = last_sequence.reshape((1, look_back, 1))\n",
    "    next_price_scaled = model.predict(last_sequence_reshaped)[0][0]\n",
    "    next_price_actual = scaler.inverse_transform([[next_price_scaled]])[0][0]\n",
    "    forecasted_prices_lstm_scaled_back.append(next_price_actual)\n",
    "    \n",
    "    last_sequence = np.append(last_sequence[1:], [[next_price_scaled]], axis=0)\n",
    "\n",
    "# Step 10: Plot actual vs predicted prices and forecasted future prices with enhancements (remains unchanged)\n",
    "plt.figure(figsize=(14,7))\n",
    "plt.plot(df['Date'][train_size + look_back:], df['Price'][train_size + look_back:], label='Actual Prices', color='blue')\n",
    "plt.plot(df['Date'][train_size + look_back:], predicted_prices_lstm_scaled_back.flatten(), label='Predicted Prices (LSTM)', color='orange')\n",
    "plt.axvline(x=df['Date'].iloc[-28], color='red', linestyle='--', label='Prediction Start')\n",
    "plt.plot(pd.date_range(df['Date'].iloc[-28], periods=28), forecasted_prices_lstm_scaled_back, label='Forecasted Prices (Next 4 weeks)', color='green')\n",
    "plt.title('LSTM Predicted Prices vs Actual Prices with Forecast')\n",
    "plt.xlabel('Date')\n",
    "plt.ylabel('Price')\n",
    "plt.legend()\n",
    "plt.grid()\n",
    "plt.tight_layout()  \n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Available GPUs: []\n",
      "Downloading data from https://www.cs.toronto.edu/~kriz/cifar-100-python.tar.gz\n",
      "\u001b[1m169001437/169001437\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m30s\u001b[0m 0us/step\n",
      "Epoch 1/5\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "\n",
    "# Check if GPU is available\n",
    "physical_devices = tf.config.list_physical_devices('GPU')\n",
    "print(\"Available GPUs:\", physical_devices)\n",
    "\n",
    "# Example model training (using CIFAR-100 dataset)\n",
    "cifar = tf.keras.datasets.cifar100\n",
    "(x_train, y_train), (x_test, y_test) = cifar.load_data()\n",
    "\n",
    "model = tf.keras.applications.ResNet50(\n",
    "    include_top=True,\n",
    "    weights=None,\n",
    "    input_shape=(32, 32, 3),\n",
    "    classes=100,\n",
    ")\n",
    "\n",
    "loss_fn = tf.keras.losses.SparseCategoricalCrossentropy(from_logits=False)\n",
    "model.compile(optimizer=\"adam\", loss=loss_fn, metrics=[\"accuracy\"])\n",
    "\n",
    "model.fit(x_train, y_train, epochs=5, batch_size=64)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Available GPUs: [PhysicalDevice(name='/physical_device:GPU:0', device_type='GPU')]\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "\n",
    "# Check if GPU is available\n",
    "physical_devices = tf.config.list_physical_devices('GPU')\n",
    "print(\"Available GPUs:\", physical_devices)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/rx/s42f27kn5j5ddqzlhz7qr2yw0000gn/T/ipykernel_12709/1602127721.py:31: FutureWarning: The argument 'date_parser' is deprecated and will be removed in a future version. Please use 'date_format' instead, or read your data in as 'object' dtype and then call 'to_datetime'.\n",
      "  df = pd.read_csv(file_path, skiprows=2,\n",
      "2024-11-17 21:49:33.390129: I metal_plugin/src/device/metal_device.cc:1154] Metal device set to: Apple M2 Max\n",
      "2024-11-17 21:49:33.390160: I metal_plugin/src/device/metal_device.cc:296] systemMemory: 32.00 GB\n",
      "2024-11-17 21:49:33.390164: I metal_plugin/src/device/metal_device.cc:313] maxCacheSize: 10.67 GB\n",
      "2024-11-17 21:49:33.390180: I tensorflow/core/common_runtime/pluggable_device/pluggable_device_factory.cc:305] Could not identify NUMA node of platform GPU ID 0, defaulting to 0. Your kernel may not have been built with NUMA support.\n",
      "2024-11-17 21:49:33.390192: I tensorflow/core/common_runtime/pluggable_device/pluggable_device_factory.cc:271] Created TensorFlow device (/job:localhost/replica:0/task:0/device:GPU:0 with 0 MB memory) -> physical PluggableDevice (device: 0, name: METAL, pci bus id: <undefined>)\n",
      "/Users/ytsang/miniforge3/envs/tf_env/lib/python3.9/site-packages/keras/src/layers/rnn/rnn.py:204: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n",
      "  super().__init__(**kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data shape after filtering: (1044, 2)\n",
      "Estimated Volatility: 1.0000\n",
      "Epoch 1/50\n"
     ]
    },
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mThe Kernel crashed while executing code in the current cell or a previous cell. \n",
      "\u001b[1;31mPlease review the code in the cell(s) to identify a possible cause of the failure. \n",
      "\u001b[1;31mClick <a href='https://aka.ms/vscodeJupyterKernelCrash'>here</a> for more info. \n",
      "\u001b[1;31mView Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from scipy.stats import norm\n",
    "from scipy.optimize import minimize\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import LSTM, Dense, Dropout\n",
    "from sklearn.metrics import mean_squared_error\n",
    "import tensorflow as tf\n",
    "\n",
    "# Step 1: Configure TensorFlow for memory management\n",
    "gpus = tf.config.list_physical_devices('GPU')\n",
    "if gpus:\n",
    "    try:\n",
    "        for gpu in gpus:\n",
    "            tf.config.experimental.set_memory_growth(gpu, True)\n",
    "    except RuntimeError as e:\n",
    "        print(e)\n",
    "\n",
    "# Set mixed precision policy for reduced memory usage and faster computation\n",
    "tf.keras.mixed_precision.set_global_policy('mixed_float16')\n",
    "\n",
    "# Step 2: Load and preprocess data with error handling and optimized date parsing\n",
    "file_path = 'gold_dec24(GC=F)_1wk.csv'\n",
    "\n",
    "def load_and_preprocess_data(file_path):\n",
    "    df = None  # Initialize df to None\n",
    "    try:\n",
    "        # Load the CSV file and parse dates\n",
    "        df = pd.read_csv(file_path, skiprows=2,\n",
    "                         names=['Date', 'Price', 'Adj Close', 'Close', 'High', 'Low', 'Open', 'Volume'],\n",
    "                         parse_dates=['Date'], \n",
    "                         date_parser=lambda x: pd.to_datetime(x.strip(), errors='coerce'))\n",
    "\n",
    "        # Filter to keep only essential columns and drop rows with NaN values in 'Price'\n",
    "        df = df[['Date', 'Price']].dropna()\n",
    "        df['Price'] = pd.to_numeric(df['Price'], errors='coerce')\n",
    "\n",
    "        # Display DataFrame shape\n",
    "        print(f\"Data shape after filtering: {df.shape}\")\n",
    "    except FileNotFoundError:\n",
    "        raise Exception(f\"File not found: {file_path}\")\n",
    "    except ValueError as e:\n",
    "        print(\"Date parsing error:\", e)\n",
    "\n",
    "    return df  # Return df, which may be None if an error occurred\n",
    "\n",
    "# Load data\n",
    "df = load_and_preprocess_data(file_path)\n",
    "\n",
    "# Step 3: Define Black-Scholes model\n",
    "def black_scholes(S, K, T, r, sigma):\n",
    "    d1 = (np.log(S / K) + (r + 0.5 * sigma ** 2) * T) / (sigma * np.sqrt(T))\n",
    "    d2 = d1 - sigma * np.sqrt(T)\n",
    "    call_price = S * norm.cdf(d1) - K * np.exp(-r * T) * norm.cdf(d2)\n",
    "    return call_price\n",
    "\n",
    "# Set up parameters for Black-Scholes\n",
    "S = df['Price'].values  \n",
    "K = np.mean(S)                  \n",
    "T = 1                           \n",
    "r = 0.01                        \n",
    "\n",
    "# Define objective function to estimate volatility\n",
    "def objective_function(sigma):\n",
    "    call_prices = black_scholes(S, K, T, r, sigma)\n",
    "    return np.mean((call_prices - S) ** 2)\n",
    "\n",
    "# Estimate volatility using optimization\n",
    "initial_sigma = 0.2\n",
    "result = minimize(objective_function, initial_sigma, bounds=[(0.01, 1)])\n",
    "estimated_sigma = result.x[0]\n",
    "\n",
    "print(f'Estimated Volatility: {estimated_sigma:.4f}')\n",
    "\n",
    "# Step 4: Prepare data for LSTM model (Deep Learning)\n",
    "scaler = MinMaxScaler(feature_range=(0, 1))\n",
    "scaled_data = scaler.fit_transform(df[['Price']])\n",
    "\n",
    "train_size = int(len(scaled_data) * 0.8)\n",
    "train_data = scaled_data[:train_size]\n",
    "test_data = scaled_data[train_size:]\n",
    "\n",
    "def create_sequences(data, look_back=60):\n",
    "    X, y = [], []\n",
    "    for i in range(look_back, len(data)):\n",
    "        X.append(data[i-look_back:i])\n",
    "        y.append(data[i])\n",
    "    return np.array(X), np.array(y)\n",
    "\n",
    "look_back = 60\n",
    "X_train, y_train = create_sequences(train_data, look_back)\n",
    "X_test, y_test = create_sequences(test_data, look_back)\n",
    "\n",
    "X_train = X_train.reshape(X_train.shape[0], X_train.shape[1], 1)\n",
    "X_test = X_test.reshape(X_test.shape[0], X_test.shape[1], 1)\n",
    "\n",
    "# Step 5: Build LSTM Model with improved architecture\n",
    "model = Sequential()\n",
    "model.add(LSTM(units=50, return_sequences=True, input_shape=(X_train.shape[1], 1)))\n",
    "model.add(Dropout(0.2))\n",
    "model.add(LSTM(units=25))  \n",
    "model.add(Dropout(0.2))\n",
    "model.add(Dense(units=1))\n",
    "\n",
    "model.compile(optimizer='adam', loss='mean_squared_error')\n",
    "\n",
    "# Step 6: Create efficient data pipelines using tf.data API\n",
    "train_dataset = tf.data.Dataset.from_tensor_slices((X_train, y_train)).batch(32).prefetch(tf.data.AUTOTUNE)\n",
    "\n",
    "# Step 7: Train the LSTM Model with early stopping\n",
    "from tensorflow.keras.callbacks import EarlyStopping\n",
    "\n",
    "early_stopping = EarlyStopping(monitor='loss', patience=5)\n",
    "model.fit(train_dataset, epochs=50, callbacks=[early_stopping])\n",
    "\n",
    "# Step 8: Predict future prices using LSTM model on test set\n",
    "predicted_prices_lstm = model.predict(X_test)\n",
    "predicted_prices_lstm_scaled_back = scaler.inverse_transform(predicted_prices_lstm)\n",
    "\n",
    "# Evaluate model performance on test set\n",
    "mse_test = mean_squared_error(df['Price'][train_size + look_back:], predicted_prices_lstm_scaled_back.flatten())\n",
    "print(f'Mean Squared Error on Test Set: {mse_test:.2f}')\n",
    "\n",
    "# Step 9: Predict next four weeks (28 days) of prices based on last known data point\n",
    "last_sequence = test_data[-look_back:]\n",
    "forecasted_prices_lstm_scaled_back = []\n",
    "for _ in range(28):  \n",
    "    last_sequence_reshaped = last_sequence.reshape((1, look_back, 1))\n",
    "    next_price_scaled = model.predict(last_sequence_reshaped)[0][0]\n",
    "    next_price_actual = scaler.inverse_transform([[next_price_scaled]])[0][0]\n",
    "    forecasted_prices_lstm_scaled_back.append(next_price_actual)\n",
    "    \n",
    "    last_sequence = np.append(last_sequence[1:], [[next_price_scaled]], axis=0)\n",
    "\n",
    "# Step 10: Plot actual vs predicted prices and forecasted future prices with enhancements\n",
    "plt.figure(figsize=(14,7))\n",
    "plt.plot(df['Date'][train_size + look_back:], df['Price'][train_size + look_back:], label='Actual Prices', color='blue')\n",
    "plt.plot(df['Date'][train_size + look_back:], predicted_prices_lstm_scaled_back.flatten(), label='Predicted Prices (LSTM)', color='orange')\n",
    "plt.axvline(x=df['Date'].iloc[-28], color='red', linestyle='--', label='Prediction Start')\n",
    "plt.plot(pd.date_range(df['Date'].iloc[-28], periods=28), forecasted_prices_lstm_scaled_back, label='Forecasted Prices (Next 4 weeks)', color='green')\n",
    "plt.title('LSTM Predicted Prices vs Actual Prices with Forecast')\n",
    "plt.xlabel('Date')\n",
    "plt.ylabel('Price')\n",
    "plt.legend()\n",
    "plt.grid()\n",
    "plt.tight_layout()  \n",
    "plt.show()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "tf_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
